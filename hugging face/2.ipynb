{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5f2c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7134514451026917 0.46875\n",
      "5 0.6531563997268677 0.65625\n",
      "10 0.5934051275253296 0.90625\n",
      "15 0.6269028782844543 0.625\n",
      "20 0.6255695819854736 0.6875\n",
      "25 0.5593469142913818 0.8125\n",
      "30 0.5501055121421814 0.8125\n",
      "35 0.5296679735183716 0.875\n",
      "40 0.549964964389801 0.8125\n",
      "45 0.4904342591762543 0.875\n",
      "50 0.5259115099906921 0.875\n",
      "55 0.4688456356525421 0.90625\n",
      "60 0.5442495346069336 0.8125\n",
      "65 0.5228625535964966 0.84375\n",
      "70 0.48210766911506653 0.9375\n",
      "75 0.47963353991508484 0.90625\n",
      "80 0.492077112197876 0.84375\n",
      "85 0.4768310487270355 0.90625\n",
      "90 0.45832765102386475 0.875\n",
      "95 0.49877506494522095 0.8125\n",
      "100 0.47444072365760803 0.90625\n",
      "105 0.4646175801753998 0.90625\n",
      "110 0.4031294584274292 0.9375\n",
      "115 0.5296034216880798 0.8125\n",
      "120 0.45010003447532654 0.96875\n",
      "125 0.4417475759983063 0.90625\n",
      "130 0.4535215497016907 0.875\n",
      "135 0.4887517988681793 0.8125\n",
      "140 0.4233162999153137 0.9375\n",
      "145 0.5114350914955139 0.84375\n",
      "150 0.4007081389427185 0.96875\n",
      "155 0.4612414538860321 0.875\n",
      "160 0.49602797627449036 0.84375\n",
      "165 0.45943114161491394 0.84375\n",
      "170 0.42321351170539856 0.9375\n",
      "175 0.49241453409194946 0.78125\n",
      "180 0.4694534242153168 0.875\n",
      "185 0.47085973620414734 0.84375\n",
      "190 0.4090508222579956 0.96875\n",
      "195 0.4670768678188324 0.875\n",
      "200 0.4890623986721039 0.84375\n",
      "205 0.4585348069667816 0.84375\n",
      "210 0.41802215576171875 0.90625\n",
      "215 0.4841561019420624 0.84375\n",
      "220 0.46057286858558655 0.84375\n",
      "225 0.43719691038131714 0.90625\n",
      "230 0.4400840401649475 0.84375\n",
      "235 0.4038792848587036 0.90625\n",
      "240 0.5520386695861816 0.71875\n",
      "245 0.4588482975959778 0.875\n",
      "250 0.4386771023273468 0.875\n",
      "255 0.49234336614608765 0.84375\n",
      "260 0.45766982436180115 0.875\n",
      "265 0.42070499062538147 0.9375\n",
      "270 0.4107743501663208 0.9375\n",
      "275 0.38355880975723267 0.96875\n",
      "280 0.4927223324775696 0.8125\n",
      "285 0.47483184933662415 0.84375\n",
      "290 0.47586789727211 0.78125\n",
      "295 0.43139272928237915 0.90625\n",
      "0\n",
      "0 0.44356948137283325 0.90625\n",
      "5 0.4219723343849182 0.875\n",
      "10 0.4085024893283844 0.9375\n",
      "15 0.4596787393093109 0.84375\n",
      "20 0.3799566626548767 0.90625\n",
      "25 0.5700715780258179 0.75\n",
      "30 0.4488756060600281 0.90625\n",
      "35 0.3981230556964874 0.96875\n",
      "40 0.5149935483932495 0.78125\n",
      "45 0.39713892340660095 0.96875\n",
      "50 0.3606511354446411 0.96875\n",
      "55 0.49779924750328064 0.8125\n",
      "60 0.442825585603714 0.90625\n",
      "65 0.4215097427368164 0.90625\n",
      "70 0.45651310682296753 0.84375\n",
      "75 0.44098961353302 0.875\n",
      "80 0.45601749420166016 0.875\n",
      "85 0.4366595447063446 0.875\n",
      "90 0.40164148807525635 0.9375\n",
      "95 0.38625404238700867 0.96875\n",
      "100 0.41105571389198303 0.90625\n",
      "105 0.4563121795654297 0.875\n",
      "110 0.4028340280056 0.90625\n",
      "115 0.41514307260513306 0.90625\n",
      "120 0.4692400395870209 0.84375\n",
      "125 0.44011399149894714 0.875\n",
      "130 0.44670572876930237 0.875\n",
      "135 0.38674095273017883 0.96875\n",
      "140 0.40287965536117554 0.90625\n",
      "145 0.5629920363426208 0.71875\n",
      "150 0.4675462543964386 0.84375\n",
      "155 0.41837799549102783 0.90625\n",
      "160 0.5035433769226074 0.8125\n",
      "165 0.400001585483551 0.90625\n",
      "170 0.45296964049339294 0.84375\n",
      "175 0.4533065855503082 0.84375\n",
      "180 0.4984242022037506 0.8125\n",
      "185 0.40600287914276123 0.9375\n",
      "190 0.4207824170589447 0.875\n",
      "195 0.42198097705841064 0.875\n",
      "200 0.42110151052474976 0.875\n",
      "205 0.43529191613197327 0.9375\n",
      "210 0.4242207407951355 0.90625\n",
      "215 0.3879357576370239 0.96875\n",
      "220 0.49746954441070557 0.84375\n",
      "225 0.40934720635414124 0.90625\n",
      "230 0.36190277338027954 0.96875\n",
      "235 0.4925806224346161 0.8125\n",
      "240 0.4137123227119446 0.90625\n",
      "245 0.43869829177856445 0.875\n",
      "250 0.4428568184375763 0.875\n",
      "255 0.4469616413116455 0.875\n",
      "260 0.4605848789215088 0.875\n",
      "265 0.5165787935256958 0.75\n",
      "270 0.4741775691509247 0.84375\n",
      "275 0.38584455847740173 0.9375\n",
      "280 0.4804980754852295 0.84375\n",
      "285 0.3724174201488495 0.9375\n",
      "290 0.3924495577812195 0.90625\n",
      "295 0.4681084156036377 0.875\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "token = BertTokenizer.from_pretrained(r\"D:\\github file\\Pytorch-LiMu\\hugging face\\model\\google-bert\\bert-base-chinese\\models--google-bert--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "premodel = BertModel.from_pretrained(r\"D:\\github file\\Pytorch-LiMu\\hugging face\\model\\google-bert\\bert-base-chinese\\models--google-bert--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\").to(device)\n",
    "epoch = 2\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self,split):\n",
    "        self.dataset = load_from_disk(r\"D:/github file/Pytorch-LiMu/hugging face/dataset\")\n",
    "        self.dataset = self.dataset[split]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = self.dataset[item]['text']\n",
    "        label = self.dataset[item]['label']\n",
    "        return text,label\n",
    "\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768,2)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = premodel(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:,0])\n",
    "        out = out.softmax(dim = 1) #(1,2)/(2,)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 自定义函数对数据进行编码\n",
    "def embedding(data):\n",
    "    sentes = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    data = token.batch_encode_plus(\n",
    "        batch_text_or_text_pairs = sentes,\n",
    "        add_special_tokens=True, #是否添加特殊token（如[CLS]和[SEP]），默认为True。\n",
    "        max_length=350,#编码的最大长度（截断或填充）。\n",
    "        truncation=True,\n",
    "        padding='max_length', \n",
    "        # truncation 是否截断到max_length（可选True/False/'longest_first'等）。\n",
    "        return_tensors=\"pt\", \n",
    "        # return_attention_mask：是否返回attention mask（默认为True）。\n",
    "        # return_token_type_ids：是否返回token type IDs（默认为True）。\n",
    "    )\n",
    "\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    token_type_ids = data[\"token_type_ids\"]\n",
    "    label = torch.LongTensor(labels)\n",
    "    return input_ids,attention_mask,token_type_ids,label\n",
    "\n",
    "train_dataset = Mydataset(\"train\")\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    collate_fn = embedding\n",
    ")\n",
    "\n",
    "model = myModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_fc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for e in range(epoch):\n",
    "    for i,(input_ids,attention_mask,token_type_ids,labels) in enumerate(train_loader):#将数据放到DEVICE上\n",
    "        input_ids, attention_mask, token_type_ids, labels = input_ids.to(device),\\\n",
    "            attention_mask.to(device),token_type_ids.to(device),labels.to(device)\n",
    "    #执行前向计算得到输出\n",
    "        out = model(input_ids,attention_mask, token_type_ids)\n",
    "        loss = loss_fc(out,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%5 ==0:\n",
    "            out = out.argmax(dim=1)\n",
    "            acc =(out == labels).sum().item()/len(labels)\n",
    "            print(i,loss.item(),acc)#保存模型参数\n",
    "    torch.save(model.state_dict(),f\"{epoch}bert.pt\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b44c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868243243243243\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "token = BertTokenizer.from_pretrained(r\"D:\\github file\\Pytorch-LiMu\\hugging face\\model\\google-bert\\bert-base-chinese\\models--google-bert--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "premodel = BertModel.from_pretrained(r\"D:\\github file\\Pytorch-LiMu\\hugging face\\model\\google-bert\\bert-base-chinese\\models--google-bert--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\").to(device)\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self,split):\n",
    "        self.dataset = load_from_disk(r\"D:/github file/Pytorch-LiMu/hugging face/dataset\")\n",
    "        self.dataset = self.dataset[split]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = self.dataset[item]['text']\n",
    "        label = self.dataset[item]['label']\n",
    "        return text,label\n",
    "\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768,2)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = premodel(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:,0])\n",
    "        out = out.softmax(dim = 1) #(1,2)/(2,)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 自定义函数对数据进行编码\n",
    "def embedding(data):\n",
    "    sentes = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    data = token.batch_encode_plus(\n",
    "        batch_text_or_text_pairs = sentes,\n",
    "        add_special_tokens=True, #是否添加特殊token（如[CLS]和[SEP]），默认为True。\n",
    "        max_length=350,#编码的最大长度（截断或填充）。\n",
    "        truncation=True,\n",
    "        padding='max_length', \n",
    "        # truncation 是否截断到max_length（可选True/False/'longest_first'等）。\n",
    "        return_tensors=\"pt\", \n",
    "        # return_attention_mask：是否返回attention mask（默认为True）。\n",
    "        # return_token_type_ids：是否返回token type IDs（默认为True）。\n",
    "    )\n",
    "\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    token_type_ids = data[\"token_type_ids\"]\n",
    "    label = torch.LongTensor(labels)\n",
    "    return input_ids,attention_mask,token_type_ids,label\n",
    "\n",
    "train_dataset = Mydataset(\"test\")\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    collate_fn = embedding\n",
    ")\n",
    "\n",
    "model = myModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_fc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "acc = 0\n",
    "total = 0\n",
    "model.load_state_dict(torch.load(\"bert.pt\"))\n",
    "model.eval()\n",
    "for i,(input_ids,attention_mask,token_type_ids,labels) in enumerate(train_loader):#将数据放到DEVICE上\n",
    "    input_ids, attention_mask, token_type_ids, labels = input_ids.to(device),\\\n",
    "        attention_mask.to(device),token_type_ids.to(device),labels.to(device)\n",
    "    #执行前向计算得到输出\n",
    "    out = model(input_ids,attention_mask, token_type_ids)\n",
    "\n",
    "\n",
    "    out = out.argmax(dim=1)\n",
    "    acc += (out == labels).sum().item()\n",
    "    total += len(labels)\n",
    "print(acc/total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiMu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

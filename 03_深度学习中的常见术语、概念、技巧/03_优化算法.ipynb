{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ab5cab",
   "metadata": {},
   "source": [
    "> 优化的目标是找到能使损失函数最小化的参数。这通常涉及到在训练数据上尽可能减少误差，也就是我们所说的训练误差。然而，深度学习的目标不仅仅是最小化训练误差，更重要的是最小化泛化误差，也就是模型在未见过的数据上的预测误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bfdbbd",
   "metadata": {},
   "source": [
    "### 挑战：局部最小值、鞍点和梯度消失\n",
    "深度学习模型的目标函数通常是非凸的，这意味着它可能有许多局部最优解。当优化算法找到一个局部最优解时，由于在该点附近梯度接近或等于零，优化算法可能会停止在那里，而无法找到全局最优解。除了局部最小值之外，鞍点是梯度消失的另⼀个原因。鞍点是指函数的所有梯度都消失但既是全局最小值也不是局部最小值的任何位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b0cea",
   "metadata": {},
   "source": [
    "### 小批量随机梯度下降\n",
    "在处理单个观测值时，我们需要执行许多单一矩阵-向量（甚至向量-向量）乘法，这在计算和深度学习框架开销上都是相当大的。这既适用于计算梯度以更新参数时，也适用于使用神经网络预测。也就是说，每当我们执行参数更新 $w \\leftarrow w - \\eta_{t}g_{t}$ 时，都会消耗大量资源。其中:\n",
    "\n",
    "$$\n",
    "g_{t} = \\partial_{w}f(x_{t},w)\n",
    "$$\n",
    "\n",
    "\n",
    "我们可以通过将这个操作应用于一个小批量观测值来提高计算效率。也就是说，我们将梯度 g 替换为一个小批量的平均梯度，而不是单个观测值的梯度。这样，梯度 g 的计算公式变为：\n",
    "\n",
    "$$\n",
    "g_{t} = \\partial_{w} \\frac{1}{|B|_{t}} \\sum_{i \\in B_{t}} f(x_{i},w)\n",
    "$$\n",
    "\n",
    "\n",
    "这种做法对梯度 g 的统计属性有两个影响：首先，由于小批量 B 中的所有元素都是从训练集中随机抽取的，因此梯度的期望保持不变。其次，方差显著降低。由于小批量梯度由 b 个独立梯度的平均值组成，其标准差降低了 $b^{-1/2}$ 。这是好事，因为这意味着更新更接近于完整的梯度。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

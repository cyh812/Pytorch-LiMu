{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0d66d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# softmax回归\n",
    "\n",
    "线性回归也是一种单层神经网络，输入有**n**个神经元，但是输出有**m**个神经元，因为它是用于分类的，输出代表不同类别的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd97d0",
   "metadata": {},
   "source": [
    "### Softmax关键公式\n",
    "1. Softmax的定义：Softmax函数将一个实数向量转换为概率分布。对于每个元素，它计算该元素的指数与所有元素的指数之和的比值。这样可以确保输出向量的所有元素都是非负的，并且总和为1，因此可以被视为概率分布\n",
    "\n",
    "$$\n",
    "\\widehat{y} = softmax(o)     \n",
    "\\widehat{y}_{j} = \\frac{exp(o_{j})}{\\sum_{k=1}exp{(o_{k})}}\n",
    "$$\n",
    "\n",
    "2. Softmax的输出就是选择最有可能的类别（取概率最大的），尽管softmax函数改变了输出向量的值，但它不改变元素之间的顺序，如（1，2，2）-（0.2，0.4，0.4）\n",
    "\n",
    "$$\n",
    "argmax_{j} \\widehat{y}_{j} = argmax_{j} o_{j}\n",
    "$$\n",
    "\n",
    "3. 交叉熵损失：在多分类问题中，模型预测的概率分布为  ，而真实的标签分布为  。交叉熵损失函数用于度量这两个分布之间的差异。公式如下：\n",
    "\n",
    "$$\n",
    "loss(y,\\widehat{y}) = - \\sum_{j=1}^{q} y_jlog{\\widehat{y}_{j}}\n",
    "$$\n",
    "\n",
    "这里的log就是ln\n",
    "\n",
    "4. 交叉熵损失的导数：交叉熵损失函数的梯度是softmax模型分配的概率与真实标签（由独热标签向量表示）之间的差异。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial{o_{j}}} loss(y,\\widehat{y}) = softmax(o)_{j} - y_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd6465",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "将展平每个图像，把它们看作长度为784的向量。\n",
    "因为我们的数据集有10个类别，所以网络输出维度为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016fe6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:36.594606Z",
     "iopub.status.busy": "2023-08-18T07:05:36.594134Z",
     "iopub.status.idle": "2023-08-18T07:05:36.599637Z",
     "shell.execute_reply": "2023-08-18T07:05:36.598552Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.306316\n",
      "epoch 2, loss 1.475313\n",
      "epoch 3, loss 0.911730\n",
      "epoch 4, loss 0.588419\n",
      "epoch 5, loss 0.410199\n",
      "epoch 6, loss 0.306664\n",
      "epoch 7, loss 0.241836\n",
      "epoch 8, loss 0.198371\n",
      "epoch 9, loss 0.167557\n",
      "epoch 10, loss 0.144721\n",
      "epoch 11, loss 0.127190\n",
      "epoch 12, loss 0.113344\n",
      "epoch 13, loss 0.102151\n",
      "epoch 14, loss 0.092927\n",
      "epoch 15, loss 0.085202\n",
      "epoch 16, loss 0.078642\n",
      "epoch 17, loss 0.073005\n",
      "epoch 18, loss 0.068111\n",
      "epoch 19, loss 0.063823\n",
      "epoch 20, loss 0.060038\n",
      "epoch 21, loss 0.056671\n",
      "epoch 22, loss 0.053658\n",
      "epoch 23, loss 0.050946\n",
      "epoch 24, loss 0.048492\n",
      "epoch 25, loss 0.046262\n",
      "epoch 26, loss 0.044227\n",
      "epoch 27, loss 0.042361\n",
      "epoch 28, loss 0.040646\n",
      "epoch 29, loss 0.039063\n",
      "epoch 30, loss 0.037598\n",
      "epoch 31, loss 0.036238\n",
      "epoch 32, loss 0.034972\n",
      "epoch 33, loss 0.033792\n",
      "epoch 34, loss 0.032688\n",
      "epoch 35, loss 0.031654\n",
      "epoch 36, loss 0.030682\n",
      "epoch 37, loss 0.029768\n",
      "epoch 38, loss 0.028907\n",
      "epoch 39, loss 0.028094\n",
      "epoch 40, loss 0.027326\n",
      "epoch 41, loss 0.026598\n",
      "epoch 42, loss 0.025907\n",
      "epoch 43, loss 0.025252\n",
      "epoch 44, loss 0.024629\n",
      "epoch 45, loss 0.024035\n",
      "epoch 46, loss 0.023470\n",
      "epoch 47, loss 0.022930\n",
      "epoch 48, loss 0.022415\n",
      "epoch 49, loss 0.021922\n",
      "epoch 50, loss 0.021450\n",
      "epoch 51, loss 0.020998\n",
      "epoch 52, loss 0.020565\n",
      "epoch 53, loss 0.020149\n",
      "epoch 54, loss 0.019750\n",
      "epoch 55, loss 0.019366\n",
      "epoch 56, loss 0.018997\n",
      "epoch 57, loss 0.018641\n",
      "epoch 58, loss 0.018299\n",
      "epoch 59, loss 0.017969\n",
      "epoch 60, loss 0.017650\n",
      "epoch 61, loss 0.017343\n",
      "epoch 62, loss 0.017046\n",
      "epoch 63, loss 0.016759\n",
      "epoch 64, loss 0.016482\n",
      "epoch 65, loss 0.016213\n",
      "epoch 66, loss 0.015953\n",
      "epoch 67, loss 0.015702\n",
      "epoch 68, loss 0.015458\n",
      "epoch 69, loss 0.015222\n",
      "epoch 70, loss 0.014992\n",
      "epoch 71, loss 0.014770\n",
      "epoch 72, loss 0.014554\n",
      "epoch 73, loss 0.014344\n",
      "epoch 74, loss 0.014140\n",
      "epoch 75, loss 0.013942\n",
      "epoch 76, loss 0.013749\n",
      "epoch 77, loss 0.013562\n",
      "epoch 78, loss 0.013380\n",
      "epoch 79, loss 0.013202\n",
      "epoch 80, loss 0.013029\n",
      "epoch 81, loss 0.012861\n",
      "epoch 82, loss 0.012697\n",
      "epoch 83, loss 0.012537\n",
      "epoch 84, loss 0.012381\n",
      "epoch 85, loss 0.012228\n",
      "epoch 86, loss 0.012080\n",
      "epoch 87, loss 0.011935\n",
      "epoch 88, loss 0.011794\n",
      "epoch 89, loss 0.011655\n",
      "epoch 90, loss 0.011520\n",
      "epoch 91, loss 0.011388\n",
      "epoch 92, loss 0.011260\n",
      "epoch 93, loss 0.011133\n",
      "epoch 94, loss 0.011010\n",
      "epoch 95, loss 0.010890\n",
      "epoch 96, loss 0.010772\n",
      "epoch 97, loss 0.010656\n",
      "epoch 98, loss 0.010543\n",
      "epoch 99, loss 0.010433\n",
      "epoch 100, loss 0.010324\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "net.apply(init_weights)\n",
    "\n",
    "# 在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 使用学习率为0.1的小批量随机梯度下降作为优化算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# 生成模拟数据\n",
    "num_train = 80  # 训练样本数\n",
    "num_test = 20   # 测试样本数\n",
    "num_classes = 10  # 标签类别数\n",
    "\n",
    "# 生成随机数据（形状：80*784的训练数据 + 20*784的测试数据）\n",
    "train_data = torch.randn(num_train, num_inputs)  # 正态分布随机数\n",
    "test_data = torch.randn(num_test, num_inputs)\n",
    "\n",
    "# 生成100*1的标签（前80个是训练标签，后20个是测试标签）\n",
    "labels = torch.randint(0, num_classes, (num_train + num_test, 1))  # 0~2的随机整数\n",
    "\n",
    "# 分离训练和测试标签\n",
    "train_labels = labels[:num_train]\n",
    "test_labels = labels[num_train:]\n",
    "\n",
    "# 训练\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    l = loss(net(train_data),train_labels.squeeze())\n",
    "    trainer.zero_grad()\n",
    "    l.backward()\n",
    "    trainer.step()\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "\n",
    "# 在测试集上的准确率\n",
    "y = test_labels.squeeze()\n",
    "y_hat = net(test_data).argmax(dim=1)\n",
    "\n",
    "acc = (y == y_hat).sum().item()/len(y)\n",
    "print(acc)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "LiMu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "required_libs": [],
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
